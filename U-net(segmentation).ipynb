{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RaDPGCHRdyt3"
      },
      "outputs": [],
      "source": [
        "#This cell imports all the necessary libraries required for building and training the U-Net model.\n",
        "#It includes PyTorch components like `torch`, `torch.nn`, and `torchvision`, along with `tqdm` for progress bars, `numpy` for numerical operations, `PIL` for image processing, and `os` for file path management.\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import BatchNorm2d\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ace8e97"
      },
      "outputs": [],
      "source": [
        "#This function defines a standard convolutional block used throughout the U-Net architecture.\n",
        "#Each block consists of two convolutional layers, each followed by Batch Normalization and a ReLU activation function.\n",
        "\n",
        "def conv(in_ch , out_ch):\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_ch , out_ch , kernel_size= 3 , padding=1),\n",
        "      nn.BatchNorm2d(out_ch),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_ch , out_ch , kernel_size= 3 , padding=1),\n",
        "      nn.BatchNorm2d(out_ch),\n",
        "      nn.ReLU(inplace=True),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7ab39025"
      },
      "outputs": [],
      "source": [
        "#This cell defines the `UNet` class, implementing the complete U-Net architecture.\n",
        "#It includes the encoder (downsampling path), a bottleneck, and the decoder (upsampling path) with skip connections.\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.down1 = conv(in_channels, 64)\n",
        "        self.down2 = conv(64, 128)\n",
        "        self.down3 = conv(128, 256)\n",
        "        self.down4 = conv(256, 512)\n",
        "\n",
        "        self.bottleneck = conv(512, 1024)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.up1 = conv(1024, 512)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.up2 = conv(512, 256)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.up3 = conv(256, 128)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.up4 = conv(128, 64)\n",
        "\n",
        "        self.outconv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.down1(x)\n",
        "        pl1 = self.pool(enc1)\n",
        "\n",
        "        enc2 = self.down2(pl1)\n",
        "        pl2 = self.pool(enc2)\n",
        "\n",
        "        enc3 = self.down3(pl2)\n",
        "        pl3 = self.pool(enc3)\n",
        "\n",
        "        enc4 = self.down4(pl3)\n",
        "        pl4 = self.pool(enc4)\n",
        "\n",
        "\n",
        "        bn = self.bottleneck(pl4)\n",
        "\n",
        "\n",
        "        dec1 = self.upconv1(bn)\n",
        "        dec1 = torch.cat([dec1, enc4], dim=1)\n",
        "        dec1 = self.up1(dec1)\n",
        "\n",
        "        dec2 = self.upconv2(dec1)\n",
        "        dec2 = torch.cat([dec2, enc3], dim=1)\n",
        "        dec2 = self.up2(dec2)\n",
        "\n",
        "        dec3 = self.upconv3(dec2)\n",
        "        dec3 = torch.cat([dec3, enc2], dim=1)\n",
        "        dec3 = self.up3(dec3)\n",
        "\n",
        "        dec4 = self.upconv4(dec3)\n",
        "        dec4 = torch.cat([dec4, enc1], dim=1)\n",
        "        dec4 = self.up4(dec4)\n",
        "\n",
        "\n",
        "        out = self.outconv(dec4)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBmDCwN5EgDE",
        "outputId": "1fb7ba59-48a5-4997-a392-94bf05339c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#This cell mounts Google Drive to the Colab environment.\n",
        "#This step is essential for accessing dataset files stored in your Google Drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hLwy8JdsafkK"
      },
      "outputs": [],
      "source": [
        "#This cell defines the directory paths for the image and mask data.\n",
        "#These paths point to the locations within Google Drive where the `CameraRGB` (input images) and `CameraMask` (segmentation masks) folders are stored.\n",
        "\n",
        "image_dir = \"/content/drive/MyDrive/U-net(files)/data/CameraRGB\"\n",
        "mask_dir = \"/content/drive/MyDrive/U-net(files)/data/CameraMask\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4d829baa"
      },
      "outputs": [],
      "source": [
        "#This cell defines image and mask transformations (resizing, converting to tensor, normalization).\n",
        "#It also defines the `UNetDataset` class for loading images and masks, and then initializes the dataset and `DataLoader` for batching data during training.\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "class UNetDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, img_transform=None, mask_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.images[index]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, img_name)\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.img_transform is not None:\n",
        "            image = self.img_transform(image)\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "dataset = UNetDataset(image_dir=image_dir, mask_dir=mask_dir, img_transform=img_transform, mask_transform=mask_transform)\n",
        "\n",
        "batch_size = 8\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8647ac3",
        "outputId": "a656696d-7623-4c73-cd82-1b4f3125a57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in the dataset: 1060\n"
          ]
        }
      ],
      "source": [
        "#dataset all images number\n",
        "\n",
        "print(f\"Number of images in the dataset: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4214b49b",
        "outputId": "0d3db2bc-f18b-4c55-96dc-a225be1550ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "#Set device to GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Initialize the U-Net model\n",
        "#Assuming input images have 3 channels (RGB) and output masks have 1 channel (binary segmentation)\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "\n",
        "#Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a34d59e",
        "outputId": "1ddcdb29-4c6b-4a87-f551-79ee0e35f4ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1: 100%|██████████| 133/133 [2:15:22<00:00, 61.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Average Loss: 0.3855\n",
            "Training complete for one epoch.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Training loop for one epoch\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() #Set the model for training\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        #Forward\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        #Backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete for one epoch.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRbuKvBPbFBh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
